{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Your architecture should consist of a single hidden layer with up to k nodes.\n",
    "- You can use any activation function (e.g., sigmoid, tanh, etc.) in the hidden nodes.\n",
    "- Your model must use a bias term at the input and hidden layers. It can be a standalone term or\n",
    "be incorporated in the weight matrices.\n",
    "- You should use gradient descent to train your FFNN.\n",
    "- You may find it helpful to use random number seeds for reproducibility when debugging.\n",
    "- You do not need to use a GPU for this assignment, and your models should train in less than one\n",
    "minute each.\n",
    "- You are responsible for selecting hyperparameters (e.g., number of hidden nodes, learning rate,\n",
    "training epochs, batch sizes, early stopping criteria, lambda, etc.). The goal is to get “good”\n",
    "performance from your model, but an exhaustive hyper-parameter search is unnecessary.\n",
    "- All code, exhibits and answers to free-response questions must be in a single Jupyter notebook.\n",
    "- Your code should use parameters to control all functionality needed to complete specific tasks\n",
    "(see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define model\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SingleLayerFFNN(nn.Module):\n",
    "    def __init__(self, hidden_size, activation_fn=\"sigmoid\", lambda_norm=1e-4, lambda_ortho=1e-4):\n",
    "        super(SingleLayerFFNN, self).__init__()\n",
    "\n",
    "        self.input_layer = nn.Linear(2, hidden_size, bias=True)\n",
    "        # self.output_layer = nn.Linear(hidden_size, 2, bias=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, 1, bias=True)\n",
    "\n",
    "        if activation_fn == \"sigmoid\":\n",
    "            self.activation = torch.sigmoid\n",
    "        elif activation_fn == \"tanh\":\n",
    "            self.activation = torch.tanh\n",
    "        elif activation_fn == \"relu\":\n",
    "            self.activation = torch.relu\n",
    "\n",
    "        self.lambda_norm = lambda_norm\n",
    "        self.lambda_ortho = lambda_ortho\n",
    "\n",
    "    def forward(self, x):\n",
    "        # One hidden layer\n",
    "        hidden = self.activation(self.input_layer(x))\n",
    "        output = torch.sigmoid(self.output_layer(hidden))\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def compute_regularization(self):\n",
    "        # first regularizer should minimize the norm of the input layer weight matrix; penalize large input layer weights\n",
    "        norm_reg = self.lambda_norm * torch.norm(self.input_layer.weight, p=\"fro\")**2\n",
    "\n",
    "        # second regularizer should encourage orthogonality in the intermediate decision boundaries learned in the first layer\n",
    "        # => encourage orthogonality = make dot product closr to zero\n",
    "        weight = self.input_layer.weight\n",
    "        dot_products = torch.mm(weight, weight.T) # dot_products[i,j] represents the dot prod between ith and jth weight vectors\n",
    "        identity = torch.eye(dot_products.size(0), device=dot_products.device) # represents the ideal orthogonal case (off-diagonal are 0)\n",
    "        # identity = torch.eye(dot_products.size(0), device=dot_products.device)\n",
    "\n",
    "        ortho_reg = self.lambda_ortho * torch.norm(dot_products - identity, p=\"fro\")**2 # compute the deviation of the dot_products from the identity\n",
    "\n",
    "        return norm_reg + ortho_reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train model\n",
    "'''\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_model(model, train_loader, num_epochs, learning_rate):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # loss for this epoch\n",
    "        epoch_loss = 0\n",
    "        for data, target in train_loader:\n",
    "            target = target.float().unsqueeze(1)\n",
    "            optimizer.zero_grad() # reset gradients\n",
    "            output = model(data) # forward pass\n",
    "            # print(output, target)\n",
    "            primary_loss = criterion(output, target) # calculate loss\n",
    "            reg_loss = model.compute_regularization()\n",
    "            loss = primary_loss + reg_loss\n",
    " \n",
    "            loss.backward() # backpropagation\n",
    "            optimizer.step() # take step based on gradients and lr\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Take in raw csv file path and batch size, return DataLoader object\n",
    "'''\n",
    "\n",
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def load_data(file_path, batch_size):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "\n",
    "    with open(file_path, \"r\") as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader) # skip header row\n",
    "        for row in reader:\n",
    "            labels.append(float(row[0]))\n",
    "            inputs.append([float(row[1]), float(row[2])])\n",
    "\n",
    "    inputs_tensor = torch.tensor(inputs, dtype=torch.float32)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    dataset = TensorDataset(inputs_tensor, labels_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.2730\n",
      "Epoch 2/100, Loss: 0.2518\n",
      "Epoch 3/100, Loss: 0.2465\n",
      "Epoch 4/100, Loss: 0.2409\n",
      "Epoch 5/100, Loss: 0.2332\n",
      "Epoch 6/100, Loss: 0.2254\n",
      "Epoch 7/100, Loss: 0.2118\n",
      "Epoch 8/100, Loss: 0.1944\n",
      "Epoch 9/100, Loss: 0.1777\n",
      "Epoch 10/100, Loss: 0.1593\n",
      "Epoch 11/100, Loss: 0.1445\n",
      "Epoch 12/100, Loss: 0.1322\n",
      "Epoch 13/100, Loss: 0.1185\n",
      "Epoch 14/100, Loss: 0.1052\n",
      "Epoch 15/100, Loss: 0.1002\n",
      "Epoch 16/100, Loss: 0.0990\n",
      "Epoch 17/100, Loss: 0.0868\n",
      "Epoch 18/100, Loss: 0.0842\n",
      "Epoch 19/100, Loss: 0.0842\n",
      "Epoch 20/100, Loss: 0.0760\n",
      "Epoch 21/100, Loss: 0.0705\n",
      "Epoch 22/100, Loss: 0.0727\n",
      "Epoch 23/100, Loss: 0.0782\n",
      "Epoch 24/100, Loss: 0.0678\n",
      "Epoch 25/100, Loss: 0.0630\n",
      "Epoch 26/100, Loss: 0.0648\n",
      "Epoch 27/100, Loss: 0.0622\n",
      "Epoch 28/100, Loss: 0.0568\n",
      "Epoch 29/100, Loss: 0.0579\n",
      "Epoch 30/100, Loss: 0.0595\n",
      "Epoch 31/100, Loss: 0.0476\n",
      "Epoch 32/100, Loss: 0.0547\n",
      "Epoch 33/100, Loss: 0.0536\n",
      "Epoch 34/100, Loss: 0.0560\n",
      "Epoch 35/100, Loss: 0.0632\n",
      "Epoch 36/100, Loss: 0.0522\n",
      "Epoch 37/100, Loss: 0.0529\n",
      "Epoch 38/100, Loss: 0.0510\n",
      "Epoch 39/100, Loss: 0.0509\n",
      "Epoch 40/100, Loss: 0.0518\n",
      "Epoch 41/100, Loss: 0.0450\n",
      "Epoch 42/100, Loss: 0.0607\n",
      "Epoch 43/100, Loss: 0.0546\n",
      "Epoch 44/100, Loss: 0.0524\n",
      "Epoch 45/100, Loss: 0.0479\n",
      "Epoch 46/100, Loss: 0.0416\n",
      "Epoch 47/100, Loss: 0.0420\n",
      "Epoch 48/100, Loss: 0.0787\n",
      "Epoch 49/100, Loss: 0.0474\n",
      "Epoch 50/100, Loss: 0.0467\n",
      "Epoch 51/100, Loss: 0.0399\n",
      "Epoch 52/100, Loss: 0.0508\n",
      "Epoch 53/100, Loss: 0.0458\n",
      "Epoch 54/100, Loss: 0.0376\n",
      "Epoch 55/100, Loss: 0.0346\n",
      "Epoch 56/100, Loss: 0.0406\n",
      "Epoch 57/100, Loss: 0.0417\n",
      "Epoch 58/100, Loss: 0.0389\n",
      "Epoch 59/100, Loss: 0.0380\n",
      "Epoch 60/100, Loss: 0.0391\n",
      "Epoch 61/100, Loss: 0.0522\n",
      "Epoch 62/100, Loss: 0.0342\n",
      "Epoch 63/100, Loss: 0.0373\n",
      "Epoch 64/100, Loss: 0.0374\n",
      "Epoch 65/100, Loss: 0.0484\n",
      "Epoch 66/100, Loss: 0.0484\n",
      "Epoch 67/100, Loss: 0.0406\n",
      "Epoch 68/100, Loss: 0.0301\n",
      "Epoch 69/100, Loss: 0.0429\n",
      "Epoch 70/100, Loss: 0.0375\n",
      "Epoch 71/100, Loss: 0.0350\n",
      "Epoch 72/100, Loss: 0.0369\n",
      "Epoch 73/100, Loss: 0.0429\n",
      "Epoch 74/100, Loss: 0.0421\n",
      "Epoch 75/100, Loss: 0.0572\n",
      "Epoch 76/100, Loss: 0.0379\n",
      "Epoch 77/100, Loss: 0.0304\n",
      "Epoch 78/100, Loss: 0.0320\n",
      "Epoch 79/100, Loss: 0.0319\n",
      "Epoch 80/100, Loss: 0.0404\n",
      "Epoch 81/100, Loss: 0.0432\n",
      "Epoch 82/100, Loss: 0.0404\n",
      "Epoch 83/100, Loss: 0.0263\n",
      "Epoch 84/100, Loss: 0.0567\n",
      "Epoch 85/100, Loss: 0.0428\n",
      "Epoch 86/100, Loss: 0.0390\n",
      "Epoch 87/100, Loss: 0.0322\n",
      "Epoch 88/100, Loss: 0.0441\n",
      "Epoch 89/100, Loss: 0.0438\n",
      "Epoch 90/100, Loss: 0.0391\n",
      "Epoch 91/100, Loss: 0.0339\n",
      "Epoch 92/100, Loss: 0.0317\n",
      "Epoch 93/100, Loss: 0.0295\n",
      "Epoch 94/100, Loss: 0.0340\n",
      "Epoch 95/100, Loss: 0.0400\n",
      "Epoch 96/100, Loss: 0.0357\n",
      "Epoch 97/100, Loss: 0.0270\n",
      "Epoch 98/100, Loss: 0.0359\n",
      "Epoch 99/100, Loss: 0.0392\n",
      "Epoch 100/100, Loss: 0.0374\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Train model with train set. \n",
    "Hyperparameters: hidden_size, batch_size, num_epochs, learning_rate\n",
    "'''\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "file_path = \"xor_train.csv\"\n",
    "\n",
    "# hidden_size from [2, 3, 5, 7, 9] \n",
    "hidden_size = 3\n",
    "batch_size = 8\n",
    "# activation from \"sigmoid\", \"relu\", \"tanh\"\n",
    "activation_fn = \"relu\"\n",
    "num_epochs = 100\n",
    "learning_rate = 0.5\n",
    "lambda_norm = 1e-4\n",
    "lambda_orth = 1e-4\n",
    "\n",
    "train_loader = load_data(file_path, batch_size)\n",
    "model = SingleLayerFFNN(hidden_size, activation_fn, lambda_norm=lambda_norm, lambda_ortho=lambda_orth)\n",
    "\n",
    "train_model(model, train_loader, num_epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            # if output exceeds threshold 0.5, classify as 1\n",
    "            preds = (output > 0.5).float()\n",
    "            all_preds.extend(preds.numpy())\n",
    "            all_targets.extend(target.numpy())\n",
    "\n",
    "    all_preds = torch.tensor(all_preds).squeeze()\n",
    "    all_targets = torch.tensor(all_targets)\n",
    "    \n",
    "    correct = (all_preds == all_targets).sum().item()\n",
    "    total = len(all_targets)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.915"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_file_path = \"xor_valid.csv\"\n",
    "eval_batch_size = 4\n",
    "\n",
    "eval_loader = load_data(eval_file_path, eval_batch_size)\n",
    "evaluate_model(model, eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_curves(model, train_loader, val_loader, num_epochs, optimizer, criterion):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for data, target in train_loader:\n",
    "            target = target.squeeze()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data).squeeze()\n",
    "            primary_loss = criterion(output, target)\n",
    "            reg_loss = model.compute_regularization()\n",
    "            loss = primary_loss + reg_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        train_loss = running_train_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                target = target.squeeze()\n",
    "                output = model(data).squeeze()\n",
    "                loss = criterion(output, target)\n",
    "                running_val_loss += loss.item()\n",
    "        val_loss = running_val_loss / len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} - Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Plotting the loss curves\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"xor_train.csv\"\n",
    "\n",
    "hidden_size = 3\n",
    "batch_size = 8\n",
    "activation_fn = \"relu\"\n",
    "num_epochs = 100\n",
    "learning_rate = 0.1\n",
    "lambda_norm = 1e-2\n",
    "lambda_orth = 1e-2\n",
    "\n",
    "eval_file_path = \"xor_valid.csv\"\n",
    "eval_batch_size = 4\n",
    "\n",
    "\n",
    "train_loader = load_data(file_path, batch_size)\n",
    "model = SingleLayerFFNN(hidden_size, activation_fn)\n",
    "eval_loader = load_data(eval_file_path, eval_batch_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "plot_loss_curves(model, train_loader, eval_loader, num_epochs, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "\n",
    "def plot_decision_surface(model, test_loader, feature_range=(-2, 2), resolution=0.01):\n",
    "    \"\"\"\n",
    "    Plots the decision surface of the model along with the test set observations.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model.\n",
    "        test_loader: DataLoader for the test set.\n",
    "        feature_range: Tuple (min, max) for the grid of features.\n",
    "        resolution: Step size for the grid.\n",
    "    \"\"\"\n",
    "    # Generate a grid of points\n",
    "    x_min, x_max = feature_range\n",
    "    y_min, y_max = feature_range\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(x_min, x_max, resolution),\n",
    "        np.arange(y_min, y_max, resolution)\n",
    "    )\n",
    "    \n",
    "    # Flatten the grid to feed into the model\n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    grid_points_tensor = torch.tensor(grid_points, dtype=torch.float32)\n",
    "    \n",
    "    # Get model predictions on the grid\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        hidden_layer_output = model.activation(model.input_layer(grid_points_tensor)) # hidden layer outputs\n",
    "        final_output = torch.sigmoid(model.output_layer(hidden_layer_output))\n",
    "    hidden_layer_outputs = (hidden_layer_output > 0.5).numpy()\n",
    "    predicted_labels = (final_output > 0.5).float().flatten()\n",
    "    \n",
    "\n",
    "    hidden_size = hidden_layer_output.shape[1]\n",
    "    for i in range(hidden_size):\n",
    "        Z_hidden = hidden_layer_outputs[:, i].reshape(xx.shape)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.contourf(xx, yy, Z_hidden, levels=50, cmap=\"coolwarm\", alpha=0.6)\n",
    "        plt.contour(xx, yy, Z_hidden, levels=[0.5], colors=\"black\", linewidths=2, linestyles=\"--\")\n",
    "        \n",
    "        # Plot test data points\n",
    "        for data, target in test_loader:\n",
    "            data = data.numpy()\n",
    "            target = target.numpy()\n",
    "            plt.scatter(data[:, 0], data[:, 1], c=target, edgecolor=\"k\", cmap=plt.cm.coolwarm, s=50)\n",
    "        \n",
    "        plt.title(f'Decision Surface of Hidden Neuron {i+1}')\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.colorbar(label='Activation Value')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    # Plot the final decision surface\n",
    "    Z_final = predicted_labels.reshape(xx.shape)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z_final, levels=50, cmap=\"coolwarm\", alpha=0.6)\n",
    "    plt.contour(xx, yy, Z_final, levels=[0.5], colors=\"black\", linewidths=2)\n",
    "    \n",
    "    # Plot test data points\n",
    "    for data, target in test_loader:\n",
    "        data = data.numpy()\n",
    "        target = target.numpy()\n",
    "        plt.scatter(data[:, 0], data[:, 1], c=target, edgecolor=\"k\", cmap=plt.cm.coolwarm, s=50)\n",
    "    \n",
    "    plt.title('Final Decision Surface')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.colorbar(label='Prediction Probability')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_file_path = \"xor_test.csv\"\n",
    "eval_batch_size = 4\n",
    "\n",
    "test_loader = load_data(eval_file_path, eval_batch_size)\n",
    "plot_decision_surface(model, test_loader, feature_range=(-4, 4), resolution=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs349",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
