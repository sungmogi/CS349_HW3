{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Your architecture should consist of a single hidden layer with up to k nodes.\n",
    "- You can use any activation function (e.g., sigmoid, tanh, etc.) in the hidden nodes.\n",
    "- Your model must use a bias term at the input and hidden layers. It can be a standalone term or\n",
    "be incorporated in the weight matrices.\n",
    "- You should use gradient descent to train your FFNN.\n",
    "- You may find it helpful to use random number seeds for reproducibility when debugging.\n",
    "- You do not need to use a GPU for this assignment, and your models should train in less than one\n",
    "minute each.\n",
    "- You are responsible for selecting hyperparameters (e.g., number of hidden nodes, learning rate,\n",
    "training epochs, batch sizes, early stopping criteria, lambda, etc.). The goal is to get “good”\n",
    "performance from your model, but an exhaustive hyper-parameter search is unnecessary.\n",
    "- All code, exhibits and answers to free-response questions must be in a single Jupyter notebook.\n",
    "- Your code should use parameters to control all functionality needed to complete specific tasks\n",
    "(see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define model\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SingleLayerFFNN(nn.Module):\n",
    "    def __init__(self, hidden_size, activation_fn=\"sigmoid\"):\n",
    "        super(SingleLayerFFNN, self).__init__()\n",
    "\n",
    "        self.input_layer = nn.Linear(2, hidden_size, bias=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, 2, bias=True)\n",
    "\n",
    "        if activation_fn == \"sigmoid\":\n",
    "            self.activation = torch.sigmoid\n",
    "        elif activation_fn == \"tanh\":\n",
    "            self.activation = torch.tanh\n",
    "        elif activation_fn == \"relu\":\n",
    "            self.activation = torch.relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        # One hidden layer\n",
    "        hidden = self.activation(self.input_layer(x))\n",
    "        output = self.output_layer(hidden)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train model\n",
    "'''\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_model(model, train_loader, num_epochs, learning_rate):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # loss for this epoch\n",
    "        epoch_loss = 0\n",
    "        for data, target in train_loader:\n",
    "            target = target.float().unsqueeze(1)\n",
    "            optimizer.zero_grad() # reset gradients\n",
    "            output = model(data) # forward pass\n",
    "            # print(output, target)\n",
    "            loss = criterion(output, target) # calculate loss\n",
    " \n",
    "            loss.backward() # backpropagation\n",
    "            optimizer.step() # take step based on gradients and lr\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Take in raw csv file path and batch size, return DataLoader object\n",
    "'''\n",
    "\n",
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def load_data(file_path, batch_size):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "\n",
    "    with open(file_path, \"r\") as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader) # skip header row\n",
    "        for row in reader:\n",
    "            labels.append(float(row[0]))\n",
    "            inputs.append([float(row[1]), float(row[2])])\n",
    "\n",
    "    inputs_tensor = torch.tensor(inputs, dtype=torch.float32)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    dataset = TensorDataset(inputs_tensor, labels_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train model with train set. \n",
    "Hyperparameters: hidden_size, batch_size, num_epochs, learning_rate\n",
    "'''\n",
    "\n",
    "file_path = \"spiral_train.csv\"\n",
    "\n",
    "# hidden_size from [2, 3, 5, 7, 9] \n",
    "hidden_size = 7\n",
    "batch_size = 8\n",
    "# activation from \"sigmoid\", \"relu\", \"tanh\"\n",
    "activation_fn = \"relu\"\n",
    "num_epochs = 300\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loader = load_data(file_path, batch_size)\n",
    "model = SingleLayerFFNN(hidden_size, activation_fn)\n",
    "\n",
    "train_model(model, train_loader, num_epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            # between the probabilities for 0 and 1, pick higher\n",
    "            preds = torch.argmax(output, dim=1)\n",
    "            all_preds.extend(preds.numpy())\n",
    "            all_targets.extend(target.numpy())\n",
    "\n",
    "    all_preds = torch.tensor(all_preds).squeeze()\n",
    "    all_targets = torch.tensor(all_targets)\n",
    "    \n",
    "    correct = (all_preds == all_targets).sum().item()\n",
    "    total = len(all_targets)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_file_path = \"spiral_test.csv\"\n",
    "eval_batch_size = 4\n",
    "\n",
    "eval_loader = load_data(eval_file_path, eval_batch_size)\n",
    "evaluate_model(model, eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_curves(model, train_loader, val_loader, num_epochs, optimizer, criterion):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for data, target in train_loader:\n",
    "            target = target.squeeze()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data).squeeze()\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        train_loss = running_train_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                target = target.squeeze()\n",
    "                output = model(data).squeeze()\n",
    "                loss = criterion(output, target)\n",
    "                running_val_loss += loss.item()\n",
    "        val_loss = running_val_loss / len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} - Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Plotting the loss curves\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"spiral_train.csv\"\n",
    "\n",
    "hidden_size = 7\n",
    "batch_size = 8\n",
    "activation_fn = \"relu\"\n",
    "num_epochs = 300\n",
    "learning_rate = 0.1\n",
    "\n",
    "eval_file_path = \"spiral_valid.csv\"\n",
    "eval_batch_size = 4\n",
    "\n",
    "\n",
    "train_loader = load_data(file_path, batch_size)\n",
    "model = SingleLayerFFNN(hidden_size, activation_fn)\n",
    "eval_loader = load_data(eval_file_path, eval_batch_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "plot_loss_curves(model, train_loader, eval_loader, num_epochs, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "\n",
    "def plot_decision_surface(model, test_loader, feature_range=(-2, 2), resolution=0.01):\n",
    "    \"\"\"\n",
    "    Plots the decision surface of the model along with the test set observations.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model.\n",
    "        test_loader: DataLoader for the test set.\n",
    "        feature_range: Tuple (min, max) for the grid of features.\n",
    "        resolution: Step size for the grid.\n",
    "    \"\"\"\n",
    "    # Generate a grid of points\n",
    "    x_min, x_max = feature_range\n",
    "    y_min, y_max = feature_range\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(x_min, x_max, resolution),\n",
    "        np.arange(y_min, y_max, resolution)\n",
    "    )\n",
    "    \n",
    "    # Flatten the grid to feed into the model\n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    grid_points_tensor = torch.tensor(grid_points, dtype=torch.float32)\n",
    "    \n",
    "    # Get model predictions on the grid\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(grid_points_tensor)\n",
    "        print(predictions)\n",
    "        predicted_labels = torch.argmax(predictions, axis=1).numpy()\n",
    "        print(predicted_labels)\n",
    "    \n",
    "    # Reshape predictions back to the grid shape\n",
    "    Z = predicted_labels.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision surface\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.coolwarm)\n",
    "    \n",
    "    # Plot test data points\n",
    "    for data, target in test_loader:\n",
    "        data = data.numpy()\n",
    "        target = target.numpy()\n",
    "        plt.scatter(data[:, 0], data[:, 1], c=target, edgecolor='k', cmap=plt.cm.coolwarm, s=50, label='Test Data')\n",
    "    \n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='o', color='w', label='Class 0',\n",
    "               markerfacecolor='red', markersize=10),\n",
    "        Line2D([0], [0], marker='o', color='w', label='Class 1',\n",
    "               markerfacecolor='blue', markersize=10)\n",
    "    ]\n",
    "\n",
    "    plt.title('Learned Decision Surface')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend(handles=legend_elements, loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_file_path = \"spiral_test.csv\"\n",
    "eval_batch_size = 4\n",
    "\n",
    "test_loader = load_data(eval_file_path, eval_batch_size)\n",
    "plot_decision_surface(model, test_loader, feature_range=(-20, 20), resolution=0.01)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
