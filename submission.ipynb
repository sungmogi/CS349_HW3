{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Your architecture should consist of a single hidden layer with up to k nodes.\n",
    "- You can use any activation function (e.g., sigmoid, tanh, etc.) in the hidden nodes.\n",
    "- Your model must use a bias term at the input and hidden layers. It can be a standalone term or\n",
    "be incorporated in the weight matrices.\n",
    "- You should use gradient descent to train your FFNN.\n",
    "- You may find it helpful to use random number seeds for reproducibility when debugging.\n",
    "- You do not need to use a GPU for this assignment, and your models should train in less than one\n",
    "minute each.\n",
    "- You are responsible for selecting hyperparameters (e.g., number of hidden nodes, learning rate,\n",
    "training epochs, batch sizes, early stopping criteria, lambda, etc.). The goal is to get “good”\n",
    "performance from your model, but an exhaustive hyper-parameter search is unnecessary.\n",
    "- All code, exhibits and answers to free-response questions must be in a single Jupyter notebook.\n",
    "- Your code should use parameters to control all functionality needed to complete specific tasks\n",
    "(see below).\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SingleLayerFFNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, activation_fn=\"sigmoid\"):\n",
    "        super(SingleLayerFFNN, self).__init__()\n",
    "\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size, bias=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size, bias=True)\n",
    "\n",
    "        if activation_fn == \"sigmoid\":\n",
    "            self.activation = torch.sigmoid\n",
    "        elif activation_fn == \"tanh\":\n",
    "            self.activation = torch.tanh\n",
    "        elif activation_fn == \"relu\":\n",
    "            self.activation = torch.relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        # One hidden layer\n",
    "        hidden = self.activation(self.input_layer(x))\n",
    "        output = self.output_layer(hidden)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train_model(model, train_loader, num_epochs, learning_rate):\n",
    "    # nn.CrossEntropyLoss for multiclass cross entropy\n",
    "    # nn.MSELoss for mean squared error\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # loss for this epoch\n",
    "        epoch_loss = 0\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "# hidden_size from [2, 3, 5, 7, 9] \n",
    "hidden_size = 2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs349",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
